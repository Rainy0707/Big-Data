{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a930eab296ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mhdfs_src_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m     \u001b[0mpartitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Raja Harsha Chinta\n",
    "Title: Alternating Least Square Implementation in Apache Spark for Product Recommendation\n",
    "Sample Call: spark-submit als_2.py AZ_P/input AmazonProductReco.dat 10 4\n",
    "\n",
    "Instructions:\n",
    "\n",
    "\tStep 1: Open Hortonworks Spark Virtual box/ spark environment with atleast Spark 1.2.1 and Python 2.7.7 installed.\n",
    "\tStep 2: If required install numpy, scipy libraries.\n",
    "\tStep 3: Execute the command \"export SPARK_HOME=/usr/hdp/2.2.4.2-2/spark\" in unix server.\n",
    "\tStep 4: Transfer the code file AmazonALS.py, ratings.dat, products.dat, users.dat to unix server.\n",
    "\tStep 5: Create a directory in HDFS like AZ/input and trandfer the .dat files to input HDFS directory created.\n",
    "\tStep 6: To run the recommendation program execute: spark-submit AmazonALS.py <InputDirectory> <OutputFileName> <Iterations> <Partitions>\n",
    "\tStep 7: If the number of iterations are more than 10, the program likely takes more than 15 minutes.\n",
    "\tStep 8: An output files is created in the same directory with userID,recommendedProduct,predictedRating\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from datetime import datetime\n",
    "from numpy.random import rand\n",
    "from numpy import matrix\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from os.path import join, isfile, dirname\n",
    "\n",
    "def loadRatings(ratingsFile):\n",
    "    \"\"\"\n",
    "    Load ratings from file.\n",
    "    \"\"\"\n",
    "    if not isfile(ratingsFile):\n",
    "        print (\"File %s does not exist.\" % ratingsFile)\n",
    "        sys.exit(1)\n",
    "    f = open(ratingsFile, 'r')\n",
    "    ratings = filter(lambda r: r[2] > 0, [parseRating(line)[1] for line in f])\n",
    "    f.close()\n",
    "    if not ratings:\n",
    "        print (\"No ratings provided.\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return ratings\n",
    "\n",
    "def parseData(line):\n",
    "    \"\"\"\n",
    "    Parses a rating record in productLens format userId,productId,rating,timestamp .\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\",\")\n",
    "    fields.drop([0], inplace=True)\n",
    "    return (int(fields[0]), int(fields[1]), fields[2])    \n",
    "\n",
    "def computeRMSE(R, us, vs):\n",
    "    \"\"\"\n",
    "    compute Root mean square error value\n",
    "    \"\"\"\n",
    "    diff = R - us * vs.T\n",
    "    rmse_val = np.sqrt(np.sum(np.power(diff, 2))/(U * V))\n",
    "    return rmse_val\n",
    "\n",
    "def updateUV(i, uv, r):\n",
    "    \"\"\"\n",
    "    Calculate updated values of U,V\n",
    "    \"\"\"\n",
    "    uu = uv.shape[0]\n",
    "    ff = uv.shape[1]\n",
    "    xtx = uv.T * uv\n",
    "    xty = uv.T * r[i, :].T\n",
    "\n",
    "    for j in range(ff):\n",
    "        xtx[j, j] += lambdas * uu\n",
    "    \n",
    "    updated_val = np.linalg.solve(xtx, xty)\n",
    "    return updated_val\n",
    "\n",
    "def updateU(i, v, r):\n",
    "    vv = v.shape[0]\n",
    "    ff = v.shape[1]\n",
    "    xtx = v.T * v\n",
    "    xty = v.T * r[i, :].T\n",
    "\n",
    "    for j in range(ff):\n",
    "        xtx[j, j] += lambdas * vv\n",
    "    \n",
    "    updated_U = np.linalg.solve(xtx, xty)\n",
    "    return updated_U\n",
    "\n",
    "def updateV(i, u, r):\n",
    "\n",
    "    uu = u.shape[0]\n",
    "    ff = u.shape[1]\n",
    "    xtx = u.T * u\n",
    "    xty = u.T * r[i, :].T\n",
    "\n",
    "    for j in range(ff):\n",
    "        xtx[j, j] += lambdas * uu\n",
    "    \n",
    "    updated_V = np.linalg.solve(xtx, xty)\n",
    "    return updated_V\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     if (len(sys.argv) != 5):\n",
    "#         print (\"USAGE: spark-submit AmazonALS.py AmznRatingsFileDir <outputfile> Iterations Partitions\")\n",
    "#         sys.exit(1)    \n",
    "    \n",
    "    # parameters are declared\n",
    "    lambdas = 0.1\n",
    "    np.random.seed(20)\n",
    "    hdfs_src_dir = sys.argv[1]\n",
    "    iterations = int(sys.argv[3]) if len(sys.argv) > 2 else 10 \n",
    "    partitions = int(sys.argv[4]) if len(sys.argv) > 3 else 4\n",
    "    start_time = datetime.now()\n",
    "    outputfile = sys.argv[2]\n",
    "\n",
    "    # AppName, memory is set to SparkContext\n",
    "    conf = SparkConf().setAppName(\"AmazonALS\").set(\"spark.executor.memory\", \"2g\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "    # ratings is an RDD of (timestamp, (userId, productId, rating))\n",
    "    ratings = sc.textFile(join(hdfs_src_dir, \"Customer.csv\")).map(parseData)\n",
    "\n",
    "    r_list = ratings.values().repartition(partitions).cache().collect()\n",
    "    r_array = np.array(r_list)\n",
    "\n",
    "    numRatings = ratings.count() \n",
    "    U = ratings.values().map(lambda r: r[0]).max() \n",
    "    V = ratings.values().map(lambda r: r[1]).max()\n",
    "    F = int(sys.argv[3]) if len(sys.argv) > 3 else 10\n",
    "    Z = np.zeros((U,V))\n",
    "    R = np.matrix(Z)\n",
    "\n",
    "    for i in range(numRatings):\n",
    "        r_local = r_array[i]\n",
    "        R[int((r_local[0]-1)),int((r_local[1]-1))] = int(r_local[2])  \n",
    "  \n",
    "    us =  matrix(rand(U, F))\n",
    "    usb = sc.broadcast(us)\n",
    "\n",
    "    vs =  matrix(rand(V, F))\n",
    "    vsb = sc.broadcast(vs)\n",
    "\n",
    "    Rb = sc.broadcast(R)\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        us = sc.parallelize(range(U), partitions).map(lambda x: updateUV(x, vsb.value, Rb.value)).collect()\n",
    "        us = matrix(np.array(us)[:, :, 0])\n",
    "        usb = sc.broadcast(us)\n",
    "\n",
    "        vs = sc.parallelize(range(V), partitions).map(lambda x: updateUV(x, usb.value, Rb.value.T)).collect()\n",
    "        vs = matrix(np.array(vs)[:, :, 0])\n",
    "        vsb = sc.broadcast(vs)\n",
    "\n",
    "        rmse_val = computeRMSE(R, us, vs)\n",
    "\n",
    "        print(\"Iteration %d:\" % i)\n",
    "        print(\"\\nRMSE: %5.4f\\n\" % rmse_val)\n",
    "    \n",
    "    reco = np.dot(us,vs.T)\n",
    "            \n",
    "    end_time = datetime.now()\n",
    "    total_time  = start_time - end_time\n",
    "    total_t = divmod(total_time.days * 86400 + total_time.seconds, 60)\n",
    "\n",
    "    print (\"---------------------------------------------------------------------\")\n",
    "    print (\"User-Product Recommendation: Predicted User-Ratings Matrix is created\") \n",
    "    print (\"---------------------------------------------------------------------\")\n",
    "    print(\"Total Minutes and Seconds: \" + str(total_t))\n",
    "\n",
    "    #l_prod = products.collect()\n",
    "    # l_users = users.collect()\n",
    "    # URatings = []\n",
    "    # preURatings = []\n",
    "\n",
    "    output = open(\"output_AmazonReco.dat\",'w')\n",
    "    print(\"File writing started\")\n",
    "\n",
    "    for i in range(U):\n",
    "        for j in range(V):\n",
    "            pRating = reco[i,j]\n",
    "            aRating = R[i,j]\n",
    "            if((aRating==0 and pRating>3)):\n",
    "                output.write(str(i)+\",\"+str(pRating)+\"\\n\")\n",
    "    \n",
    "    output.close()\n",
    "\n",
    "    \"\"\"\n",
    "   \t     # Calcuate Average rating of users before and after prediction for testing purpose\n",
    "\n",
    "             # if(aRating!=0):\n",
    "\t         # URatings.append(aRating)\n",
    "\t\t # preURatings.append(pRating)\n",
    "\n",
    "    # avgURating = float(sum(URatings))/float(len(uRatings))\n",
    "    # avgPRating = float(sum(preURatings))/float(len(preURatings))\n",
    "\n",
    "    # print (\"Avg User Rating: \", avgURating)\n",
    "    # print (\"Avg Predicted User Rating: \", avgPRating )\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    end_time = datetime.now()\n",
    "    total_time  = start_time - end_time\n",
    "    total_t = divmod(total_time.days * 86400 + total_time.seconds, 60)\n",
    " \n",
    "    print (\"-------------------------------------------------\") \n",
    "    print (\"User-Product Recommendation: File Write Completed\")\n",
    "    print (\"-------------------------------------------------\") \n",
    "    print(\"Total Minutes and Seconds: \" + str(total_t))\n",
    "\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

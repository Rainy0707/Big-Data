{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:65333)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\1\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\1\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:65333)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\1\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\1\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "# spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# data science imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit,CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkConf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-07c8647b7651>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# spark config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AmazonALS\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.executor.memory\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"2g\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# spark = SparkSession \\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkConf' is not defined"
     ]
    }
   ],
   "source": [
    "# spark config\n",
    "\n",
    "conf = SparkConf().setAppName(\"AmazonALS\").set(\"spark.executor.memory\", \"2g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"movie recommendation\") \\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"96g\") \\\n",
    "#     .config(\"spark.driver.memory\", \"96g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"8g\") \\\n",
    "#     .config(\"spark.master\", \"local[12]\") \\\n",
    "#     .getOrCreate()\n",
    "# # get spark context\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:\\\\Users\\\\1\\\\Documents\\\\WSU\\\\big data\\\\Project\\\\Source code\\\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = spark.read.load(os.path.join(data_path, 'Customer.csv'), format='csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+\n",
      "|      ASIN|    CustomerID|rating|\n",
      "+----------+--------------+------+\n",
      "|0827229534|A2JW67OY8U6HHK|     5|\n",
      "|0827229534|A2VE83MZF98ITY|     5|\n",
      "|0738700797|A11NCO6YTE4BTJ|     5|\n",
      "|0738700797| A9CQ3PLRNIR83|     4|\n",
      "|0738700797|A13SG9ACZ9O5IM|     5|\n",
      "|0738700797|A1BDAI6VEYMAZA|     5|\n",
      "|0738700797|A2P6KAWXJ16234|     4|\n",
      "|0738700797| AMACWC3M7PQFR|     4|\n",
      "|0738700797|A3GO7UV9XX14D8|     4|\n",
      "|0738700797|A1GIL64QK68WKL|     5|\n",
      "|0738700797| AEOBOF2ONQJWV|     5|\n",
      "|0738700797|A3IGHTES8ME05L|     5|\n",
      "|0738700797|A1CP26N8RHYVVO|     1|\n",
      "|0738700797| ANEIANH0WAT9D|     5|\n",
      "|0486287785|A3IDGASRQAW8B2|     5|\n",
      "|0842328327|A2591BUPXCS705|     4|\n",
      "|0486220125| ATVPDKIKX0DER|     5|\n",
      "|0486220125| AUEZ7NVOEHYRY|     5|\n",
      "|0486220125| ATVPDKIKX0DER|     5|\n",
      "|0486220125| AJYG6ZJUQPZ9M|     4|\n",
      "+----------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------+------------------+\n",
      "|summary|                ASIN|    CustomerID|            rating|\n",
      "+-------+--------------------+--------------+------------------+\n",
      "|  count|             7593107|       7593107|           7593107|\n",
      "|   mean|1.1891306393719952E9|          null| 4.178366115478156|\n",
      "| stddev|1.6494293078898814E9|          null|1.2500716519653887|\n",
      "|    min|          0001047655|A10003PM9DTGHQ|                 1|\n",
      "|    max|          B00009P1O5| AZZZZW74AAX75|                 5|\n",
      "+-------+--------------------+--------------+------------------+\n",
      "\n",
      "root\n",
      " |-- ASIN: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.describe().show()\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:65333)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\1\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\1\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:65333)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1114\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e73d4146037f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# transform asin and user alphanumeric string to index using spark StringIndexer function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0masinIndexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ASIN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"item\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhandleInvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# create indexer for asins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0muserIndexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'CustomerID'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'userid'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhandleInvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# create indexer for user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0masinIndexed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masinIndexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# apply asin indexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0muserIndexed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muserIndexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masinIndexed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masinIndexed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# apply user indexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python\\lib\\site-packages\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python\\lib\\site-packages\\pyspark\\ml\\feature.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputCol, outputCol, inputCols, outputCols, handleInvalid, stringOrderType)\u001b[0m\n\u001b[0;32m   3620\u001b[0m         \"\"\"\n\u001b[0;32m   3621\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStringIndexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3622\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"org.apache.spark.ml.feature.StringIndexer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3623\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3624\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[1;34m(java_class, *args)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mjava_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mjava_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1690\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[0;32m   1693\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1029\u001b[0m          \u001b[1;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m         \"\"\"\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    977\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    980\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    983\u001b[0m         connection = GatewayConnection(\n\u001b[0;32m    984\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[1;32m--> 985\u001b[1;33m         \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1125\u001b[0m                 \u001b[1;34m\"server ({0}:{1})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1127\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:65333)"
     ]
    }
   ],
   "source": [
    "# transform asin and user alphanumeric string to index using spark StringIndexer function\n",
    "asinIndexer = StringIndexer(inputCol=\"ASIN\", outputCol=\"item\",handleInvalid='error') # create indexer for asins\n",
    "userIndexer = StringIndexer(inputCol='CustomerID',outputCol='userid',handleInvalid='error') # create indexer for user\n",
    "asinIndexed = asinIndexer.fit(ratings).transform(ratings) # apply asin indexer\n",
    "userIndexed = userIndexer.fit(asinIndexed).transform(asinIndexed) # apply user indexer\n",
    "df_indexed = userIndexed.drop('ASIN').drop('CustomerID') # remove old columns with alphanumeric strings\n",
    "\n",
    "# 70-30 train-test split\n",
    "(df_train, df_test) = df_indexed.randomSplit([0.7,0.3])\n",
    "# cache them in memory across clusters since we access this data frequently \n",
    "df_train.cache() \n",
    "df_test.cache()\n",
    "\n",
    "# Display dataset size\n",
    "print('Train set size: {}'.format(df_train.count()))\n",
    "print('Test set size: {}'.format(df_test.count()))\n",
    "\n",
    "print('Matrix size, percentage of matrix filled and number of distinct users and itmes:')\n",
    "# calculate percentage of the user-item matrix that is filled\n",
    "df_train.createOrReplaceTempView('df_train')\n",
    "spark.sql(\"\"\"\n",
    "      SELECT *, 100 * rating/matrix_size AS percentage\n",
    "        FROM (\n",
    "          SELECT userid, item, rating, userid * item AS matrix_size\n",
    "            FROM(\n",
    "              SELECT COUNT(*) AS rating, COUNT(DISTINCT(item)) AS item, COUNT(DISTINCT(userid)) AS userid\n",
    "                FROM df_train\n",
    "                )\n",
    "            )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz = df_indexed.sample(False,0.09) # sample a small portion of the dataset for visualization\n",
    "pdf = df_viz.toPandas() # convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numuniquser = pdf['userid'].value_counts().count() # to set axis\n",
    "numuniqitem = pdf['item'].value_counts().count() # to set axis\n",
    "custompal = sns.xkcd_palette(['red', 'orange', 'sandy yellow', 'yellowgreen', 'vibrant green']) # traffic-light style palette\n",
    "scplot = sns.lmplot('userid','item',pdf,hue='rating',fit_reg=False,size=10 # use seaborn lmplot to plot user vs item and rating as hue\n",
    "           , aspect=2,palette=custompal,scatter_kws={'alpha':0.5})\n",
    "axes = scplot.axes\n",
    "axes[0,0].set_ylim(0,numuniqitem)  \n",
    "axes[0,0].set_xlim(0,numuniquser) \n",
    "scplot\n",
    "plt.savefig('scatterplot.png',dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed.createOrReplaceTempView('df_ind') # create temp SQL view\n",
    "# count number of ratings in each category \n",
    "ratingcount = spark.sql(\"\"\"\n",
    "      SELECT COUNT(rating) as count\n",
    "      ,rating\n",
    "      FROM df_ind\n",
    "      GROUP BY rating\n",
    "\"\"\")\n",
    "pandas_rc = ratingcount.toPandas()  # convert to pandas\n",
    "pandas_rc.sort_values('rating',axis=0,inplace=True) # sort \n",
    "pandas_rc.plot(x='rating',y='count',kind='bar',legend=False,color=custompal,figsize=(8,5)) # plot using the traffic-light palette\n",
    "plt.savefig('barchart.png',dpi=70) # save to disk\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = float(df_train.describe().toPandas()['rating'][1]) # mean\n",
    "print('Training set mean: {}'.format(mean))\n",
    "print('Test set baseline MSE and RMSE')     \n",
    "se_rdd = df_test.rdd.map(lambda x: (x[0]-mean)**2) #  squared error\n",
    "row = Row(\"val\") # create row\n",
    "se_df = se_rdd.map(row).toDF() # convert to df\n",
    "se_df.createOrReplaceTempView('se_df') # create temp SQL view\n",
    "baseline = spark.sql('SELECT AVG(val) as MSE,SQRT(AVG(val)) as RMSE  FROM se_df') # calculate MSE and RMSE\n",
    "baseline.show()\n",
    "baseline_rmse = float(baseline.toPandas()['RMSE'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define overall pipeline \n",
    "\n",
    "def als_pipeline(df_train,df_test,trainingdownsampling=0.99):\n",
    "    \"\"\"\n",
    "      Args: \n",
    "        df_train: pyspark train dataframe  \n",
    "        df_test: pyspark test dataframe\n",
    "        trainingdownsampling: percentage of full training set\n",
    "      Returns:\n",
    "        testset_rmse\n",
    "        baseline_rmse\n",
    "        wallclock\n",
    "    \"\"\"\n",
    "\n",
    "    # model\n",
    "    als = ALS(userCol=\"userid\", itemCol=\"item\", ratingCol=\"rating\",coldStartStrategy='drop',nonnegative=False)\n",
    "    \n",
    "    # evaluator\n",
    "    rmseevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "    # parameter grid\n",
    "    paramGrid = ParamGridBuilder()\\\n",
    "        .addGrid(als.rank, [1, 5, 10,50,70]) \\\n",
    "        .addGrid(als.maxIter, [15])\\\n",
    "        .addGrid(als.regParam, [0.05, 0.1, 0.5,5])\\\n",
    "        .build()\n",
    "\n",
    "    # train validation split\n",
    "    tvs = TrainValidationSplit(estimator=als,\n",
    "                               estimatorParamMaps=paramGrid,\n",
    "                               evaluator=rmseevaluator,\n",
    "                               trainRatio=0.8)\n",
    "\n",
    "    \n",
    "    # sample, Note : spark sample does is not guaranteed to provide exactly the fraction specified of the total\n",
    "    training = df_train.sample(False,trainingdownsampling)\n",
    "    print('Full training set size: {}'.format(df_train.count()))\n",
    "    print('Downsampled training set size: {} \\n'.format(training.count()))\n",
    "  \n",
    "    # fit model and time it\n",
    "    print('Fitting model...')\n",
    "    startTime = time()\n",
    "    tvsmodel = tvs.fit(training)\n",
    "    endTime = time()\n",
    "    wallclock = ( endTime - startTime )\n",
    "    \n",
    "    print('Wall-clock time: {}'.format(wallclock))\n",
    "    \n",
    "    print('\\n')\n",
    "    paramMap = list(zip(tvsmodel.validationMetrics,tvsmodel.getEstimatorParamMaps())) # zip validation rmse and selected parameters\n",
    "    paramMax = min(paramMap)\n",
    "    print('Best parameters and validation set RMSE:')\n",
    "    print(paramMax)\n",
    "    print('\\n')\n",
    "    \n",
    "    # predict and evaluate test set\n",
    "    predictions = tvsmodel.transform(df_test)\n",
    "    testset_rmse = rmseevaluator.evaluate(predictions)\n",
    "    print('Test set RMSE: {}'.format(testset_rmse))    \n",
    "    return testset_rmse,wallclock,paramMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsamples = [0.01,0.1,0.5,0.8] # list of percentages to downsample training set\n",
    "rmses = [] \n",
    "wallclocks = []\n",
    "params = []\n",
    "# loop through the list and apply the pipeline function, append the results to the above empty lists\n",
    "for s in downsamples:\n",
    "    print('Fitting als model for {} % of the training set'.format(s*100))\n",
    "    test_rmse,wallclock,parammax = als_pipeline(df_train,df_test,trainingdownsampling=s)\n",
    "    rmses.append(test_rmse)\n",
    "    wallclocks.append(wallclock)\n",
    "    params.append(parammax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "als = ALS(userCol=\"userid\", itemCol=\"item\", ratingCol=\"rating\",coldStartStrategy='drop',nonnegative=False)\n",
    "     \n",
    "# evaluator\n",
    "rmseevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# parameter grid\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(als.rank, [1, 5, 10,50,70]) \\\n",
    "    .addGrid(als.maxIter, [15])\\\n",
    "    .addGrid(als.regParam, [0.05, 0.1, 0.5,5])\\\n",
    "    .build()\n",
    "\n",
    "# train validation split\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=rmseevaluator,\n",
    "                           trainRatio=0.8)\n",
    "# fit model and time\n",
    "startTime = time()\n",
    "tvsmodel = tvs.fit(df_train)\n",
    "endTime = time()\n",
    "wallclock = ( endTime - startTime )\n",
    "    \n",
    "print('Wall-clock time: {}'.format(wallclock))\n",
    "    \n",
    "print('\\n')\n",
    "# zip train validation and parameter into one list\n",
    "paramMap = list(zip(tvsmodel.validationMetrics,tvsmodel.getEstimatorParamMaps()))\n",
    "paramMax = min(paramMap)\n",
    "print(paramMax)\n",
    "\n",
    "# predict and evaluate test set\n",
    "predictions = tvsmodel.transform(df_test)\n",
    "testset_rmse = rmseevaluator.evaluate(predictions)\n",
    "print('Test set RMSE: {}'.format(testset_rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append with full train set\n",
    "rmses.append(testset_rmse)\n",
    "wallclocks.append(wallclock)\n",
    "params.append(paramMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting terms separately since dictionaries are only ordered in Python 3.6 onwards, we are still on Python 3.5\n",
    "rank1 = list(params[0][1].values())[0]\n",
    "iter1 = list(params[0][1].values())[1]\n",
    "reg1 = list(params[0][1].values())[2]\n",
    "\n",
    "rank2 = list(params[1][1].values())[1]\n",
    "iter2 = list(params[1][1].values())[2]\n",
    "reg2 = list(params[1][1].values())[0]\n",
    "\n",
    "rank3 = list(params[2][1].values())[0]\n",
    "iter3 = list(params[2][1].values())[2]\n",
    "reg3 = list(params[2][1].values())[1]\n",
    "\n",
    "rank4 = list(params[3][1].values())[1]\n",
    "iter4 = list(params[3][1].values())[0]\n",
    "reg4 = list(params[3][1].values())[2]\n",
    "\n",
    "rank5 = list(params[4][1].values())[0]\n",
    "iter5 = list(params[4][1].values())[1]\n",
    "reg5 = list(params[4][1].values())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lists\n",
    "val_rmse = list(map(lambda x: x[0], params))\n",
    "ranks = [rank1,rank2,rank3,rank4,rank5]\n",
    "iters = [iter1,iter2,iter3,iter4,iter5]\n",
    "regParams = [reg1,reg2,reg3,reg4,reg5]\n",
    "#downsamples.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_results = pd.DataFrame(\n",
    "    {'Downsample percentage': downsamples,\n",
    "     'Number of latent factors': ranks,\n",
    "     'Maximum number of iterations': iters,\n",
    "     'Regularization parameter': regParams,\n",
    "     'Wall-clock time': wallclocks,\n",
    "     'Validation RMSE': val_rmse,\n",
    "     'Test RMSE': rmses\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe\n",
    "pd_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "pd_results.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "def recommendGames(model, user, num_rec):\n",
    "    # Create a dataset with distinct games as one column and the user of interest as another column\n",
    "    itemsuser = df_train.select(\"item\").distinct().withColumn(\"userid\", lit(user))\n",
    "    #itemsuser.show(n=5)\n",
    "\n",
    "    # filter out games that user has already rated \n",
    "    gamesrated = df_train.filter(df_train.userid == user).select(\"item\", \"userid\")\n",
    "\n",
    "    # apply trained recommender system\n",
    "    predictions = model.transform(itemsuser.subtract(gamesrated)).dropna().orderBy(\"prediction\", ascending=False).limit(num_rec).select(\"item\", \"prediction\")\n",
    "    predictions.show()\n",
    "    \n",
    "    # convert index back to original ASIN \n",
    "    converter = IndexToString(inputCol=\"item\", outputCol=\"originalCategory\")\n",
    "    converted = converter.transform(predictions)\n",
    "    converted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pick a random user id (696) and display 3 recommendations\n",
    "recommendGames(tvsmodel,696,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
